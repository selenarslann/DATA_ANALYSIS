{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63bc4770",
   "metadata": {},
   "source": [
    "## SCRAPING REDDIT DATA WITH PMAW LIBRARY FOR SPECIFIC KEYWORD "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d59277",
   "metadata": {},
   "source": [
    "In this script, we will be using the pmaw library to scrape Reddit data for a specific keyword. The data will be collectingfor the keyword \"postpartum\" within the year 2021. PMAW (Pushshift API Wrapper) is a Python wrapper for the Pushshift.io API. It allows for easy access to the data provided by the API, and provides a variety of useful functions for searching and filtering the data.\n",
    "\n",
    "Before we begin, it's important to understand the structure of Reddit. Reddit is an online platform that allows users to submit, discuss and share content. The content can be organized in to different communities called \"subreddits\". Each subreddit is focused on a specific topic, and users can submit posts or comments related to that topic. A submission on Reddit refers to a post that is submitted to a subreddit by a user. A comment, on the other hand, refers to a reply or discussion on a submission. In this script, we will be scraping the submission data (post) and comment data from Reddit.\n",
    "\n",
    "\n",
    "Let's begin by importing the necessary libraries including Pandas, Numpy, datetime, and the PushshiftAPI library. We also install pmaw library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15827650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install pmaw pandas\n",
    "from pmaw import PushshiftAPI\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ast\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bea411",
   "metadata": {},
   "source": [
    "## SCRAPPING POST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cdeeb3",
   "metadata": {},
   "source": [
    "We then use the PushshiftAPI to search for submissions that contain the keyword \"postpartum\" within the year 2021. The search_submissions() method is used to search for the submissions, and the 'q' parameter is used to specify the keyword to search for. The 'after' and 'before' parameters are used to specify the date range, and the 'limit' parameter is used to specify the maximum number of submissions to return. The data is stored in a dataframe called \"df_post\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ec91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = PushshiftAPI()\n",
    "end_epoch = int(dt.datetime(2021,12,31,0,0).timestamp())\n",
    "start_epoch = int(dt.datetime(2021,1,1,0,0).timestamp())\n",
    "submissions = api.search_submissions(q='postpartum', after=start_epoch, before=end_epoch,limit=30000)\n",
    "df_post=pd.DataFrame(submissions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385b5244",
   "metadata": {},
   "source": [
    "Now we will start cleaning the data. We will first replace empty selftext rows with NaN.\n",
    "We then proceed to drop the selftext rows that contain NaN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caef1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df_post.selftext.replace('',np.nan,regex = True)\n",
    "df_post = df_post[df_post['selftext'].notna()].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b5c4ce",
   "metadata": {},
   "source": [
    "We also drop any duplicated selftext rows and keep the first one. This step is done because some of the posts are shared in different subreddits, which results in duplicate entries in our dataframe. By keeping only one instance of the duplicated post, we can ensure that our data is clean and accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798f2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicated selftext rows and keep first one\n",
    "df_post = df_post.drop_duplicates(subset=[\"selftext\"], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76237e8",
   "metadata": {},
   "source": [
    "We also drop any rows that contain the selftext \"[deleted]\" or \"[removed]\" as they do not contain any relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9eebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post.drop(df_post[df_post['selftext']=='[deleted]'].index, inplace = True)\n",
    "df_post.drop(df_post[df_post['selftext']=='[removed]'].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca81702",
   "metadata": {},
   "source": [
    "Finally, we save the cleaned dataframe to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b9019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post.to_csv('YOUR PATH/df_post_original.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c22205",
   "metadata": {},
   "source": [
    "With this script, we have successfully scraped Reddit data for the keyword \"postpartum\" within the year 2021 and cleaned the data to obtain a useful dataset for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c47c2a4",
   "metadata": {},
   "source": [
    "## SCRAPPING COMMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85475152",
   "metadata": {},
   "source": [
    "This code block is used to scrape comment data from Reddit that contain the keyword \"postpartum\" within the year 2021. We are using the same PushshiftAPI library that we used in the previous code block to scrape the submission data. The api.search_comments() method is used to search for comments that contain the keyword \"postpartum\". The 'q' parameter is used to specify the keyword to search for, the 'after' and 'before' parameters are used to specify the date range and the 'limit' parameter is used to specify the maximum number of comments to return.\n",
    "\n",
    "Once we have the comments, we are creating a dataframe from the comments and storing the data in the variable \"comment_df\". Then we save the comment dataframe to a csv file. It's similar to our first code block where we are scraping post data but this time we are scraping comments instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a2e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment = api.search_comments(q='postpartum', after=start_epoch, before=end_epoch,limit=70000)\n",
    "comment_df=pd.DataFrame(comment)\n",
    "comment_df.to_csv('YOUR PATH/df_comment_original.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952339ef",
   "metadata": {},
   "source": [
    "## MATCHING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36911831",
   "metadata": {},
   "source": [
    "This code block is used to match the comments with the related posts. In order to match the comments with the related posts, we first need to create a common identifier that we can use to join the two dataframes. The 'parent_id column' in the comment dataframe contains the id of the post that the comment is related to, but it has a prefix of \"t3_\" which we need to remove. We use the .str[] method to remove the prefix and store the cleaned parent id in a new column called parent_id_clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ec4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df['parent_id_clean'] = comment_df['parent_id'].str[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36520757",
   "metadata": {},
   "source": [
    "We make a copy of post dataframe and comment dataframe and store in \"post_df1\" and \"comment_df1\" respectively. These copies are used to avoid modifying the original dataframes. We create a dataframe from the parent_id_clean column of the comment dataframe and rename it to \"id\" using the .rename() method. We also create a dataframe from the id column of the post dataframe. Then we use the pd.merge() function to join these two dataframes on the 'id' column, so we have all the comments that match with their parent posts.We then drop the duplicate rows and reset the index of the resulting dataframe, and store it in the variable \"merged_id\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6837ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df1=post_df.copy()\n",
    "comment_df1=comment_df.copy()\n",
    "parent_id=pd.DataFrame(comment_df1['parent_id_clean'])\n",
    "parent_id.rename(columns = {'parent_id_clean':'id'}, inplace = True)\n",
    "post_id=pd.DataFrame(post_df1['id'])\n",
    "merged_id=pd.merge(parent_id,post_id,how='inner',on=['id'])\n",
    "merged_id=merged_id.id.drop_duplicates().reset_index(drop=True)\n",
    "merged_id=pd.DataFrame(merged_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab371823",
   "metadata": {},
   "source": [
    "Then we will create the post and comment datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2549fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_post=pd.merge(post_df1,merged_id,how='inner',on=['id'])\n",
    "merged_id['parent_id_clean']=merged_id.id.copy()\n",
    "merged_comment=pd.merge(comment_df1,merged_id,how='inner',on=['parent_id_clean'])\n",
    "merged_comment['type']='comment'\n",
    "merged_post['type']='post'\n",
    "merged_comment.to_csv('YOUR PATH/matched_comment.csv')\n",
    "merged_post.to_csv('YOUR PATH/matched_post.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ddbec",
   "metadata": {},
   "source": [
    "In conclusion, this script uses the pmaw library to scrape submission data from Reddit for the keyword \"postpartum\" within a specific date range. The data is stored in a dataframe and cleaned to remove any duplicates or irrelevant data. Additionally, the script also scraps the comment data and matches it with the related post data to create two final datasets, one for posts and one for comments. These datasets can then be used for further analysis or modeling. Overall, this script provides a useful tool for collecting and cleaning data from Reddit for specific keywords and date ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2eb61e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
